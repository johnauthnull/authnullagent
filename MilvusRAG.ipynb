{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johnauthnull/authnullagent/blob/main/MilvusRAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "gD42mA9OdP3u"
      },
      "source": [
        "# Retrieval-Augmented Generation (RAG) with Milvus and LangChain\n",
        "\n",
        "This guide demonstrates how to build a Retrieval-Augmented Generation (RAG) system using LangChain and Milvus.\n",
        "\n",
        "The RAG system combines a retrieval system with a generative model to generate new text based on a given prompt. The system first retrieves relevant documents from a corpus using Milvus, and then uses a generative model to generate new text based on the retrieved documents.\n",
        "\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "Before running this notebook, make sure you have the following dependencies installed:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "GROQ_API_KEY=userdata.get('GROQ_API_KEY')\n",
        "OPENAIKEY=userdata.get('OPENAIKEY')"
      ],
      "metadata": {
        "id": "G9i7CMreBQ3Z"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNCxi0CldP3v",
        "outputId": "238e00fe-eb7f-4949-d115-8449e8a8616a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.4/1.0 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m408.7/408.7 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m737.4/737.4 kB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.3/119.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.6/389.6 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.1/201.1 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "! pip install --upgrade --quiet  langchain langchain-core langchain-community langchain-text-splitters langchain-milvus langchain-openai bs4 langchain-groq jq langgraph"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### I am not going to be working with Ollama since this is Google Colab and working with Ollama on this is a pain. Doing Initial tesing with Groq's inference platform instead"
      ],
      "metadata": {
        "id": "Ke6Dpgl5rC7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"GROQ_API_KEY\"]=GROQ_API_KEY\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "llm = ChatGroq(\n",
        "    model=\"llama-3.2-1b-preview\",\n",
        "    temperature=0.0,\n",
        "    max_retries=2,\n",
        "    # other params...\n",
        ")"
      ],
      "metadata": {
        "id": "OIVOhOvRsW-V"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "9EhiosJXdP3w"
      },
      "source": [
        "> If you are using Google Colab, to enable dependencies just installed, you may need to **restart the runtime** (click on the \"Runtime\" menu at the top of the screen, and select \"Restart session\" from the dropdown menu)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "t8v7xJtldP3x"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAIKEY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "_r_qVenudP3x"
      },
      "source": [
        "\n",
        "## Prepare the data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qeaf7pLOdP3x",
        "outputId": "e3a4b1f0-f021-48e5-cec2-473b9520fffb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"AccessMedium\": \"\", \"AdDomain\": \"\", \"AdOu\": \"RDP\", \"AdUser\": 156624, \"AdUserMatch\": \"muthu\", \"AdUserType\": 105, \"DestinationEndpoint\": \"\", \"DestinationEndpointIp\": 0, \"DestinationEndpointMatch\": \"\", \"EndpointUser\": \"AD-LOCAL\", \"EndpointUserMatch\": \"exact\", \"EndpointUserType\": \"windows\", \"LogID\": \"exact\", \"LogType\": \"\", \"LoginStatus\": \"34.67.34.186\", \"OrgID\": \"AD\", \"Os\": \"exact\", \"Protocol\": \"\", \"SamAccountName\": \"DIRECT\", \"SourceEndpoint\": 1, \"SourceEndpointIP\": \"\", \"SourceEndpointMatch\": \"user\", \"SourceEndpointType\": \"endpoint\", \"TenantID\": \"user\", \"TimeStamp1\": \"exact\", \"VectorData\": \"success\"}\n",
            "{'source': '/content/exported_data.json', 'seq_num': 31}\n",
            "{\"id\": 1, \"org_id\": 105, \"tenant_id\": 1, \"src_ip\": \"154.213.185.141\", \"dest_ip\": \"35.238.119.241\", \"user_name\": \"sonar\", \"service\": \"sshd[1173994]\", \"login_status\": \"Failure\", \"message\": \"Invalid user sonar from 154.213.185.141 port 54214\", \"count\": 1, \"created_at\": 1727177088, \"component\": \"linux-auth\", \"host_name\": \"dit-pam-config\"}\n",
            "{'source': '/content/endpointauthlogs.json', 'seq_num': 1}\n",
            "{\"policy_type\": \"ad\", \"ad\": {\"ous\": {\"ou1\": [\"all\"], \"ou2\": [\"specific_ad_group1\", \"specific_ad_group2\"]}}, \"permissions\": {\"allowed\": true, \"iam_users\": [\"user1\", \"user2\"], \"iam_groups\": [\"domain_admins\"]}}\n",
            "{'source': '/content/adpolicy.json', 'seq_num': 1}\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import JSONLoader\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "file_path='exported_data.json'\n",
        "data = json.loads(Path(file_path).read_text())\n",
        "loader = JSONLoader(\n",
        "         file_path=file_path,\n",
        "         jq_schema='.[]',\n",
        "         text_content=False)\n",
        "\n",
        "docs = loader.load()\n",
        "print(docs[30].page_content)\n",
        "print(docs[30].metadata)\n",
        "\n",
        "file2=\"endpointauthlogs.json\"\n",
        "data = json.loads(Path(file2).read_text())\n",
        "loader_ = JSONLoader(\n",
        "         file_path=file2,\n",
        "         jq_schema='.[]',\n",
        "         text_content=False)\n",
        "\n",
        "docs2 = loader_.load()\n",
        "print(docs2[0].page_content)\n",
        "print(docs2[0].metadata)\n",
        "\n",
        "\n",
        "file3=\"adpolicy.json\"\n",
        "data = json.loads(Path(file2).read_text())\n",
        "loader3 = JSONLoader(\n",
        "         file_path=file3,\n",
        "         jq_schema='.[]',\n",
        "         text_content=False)\n",
        "\n",
        "docs3 = loader3.load()\n",
        "print(docs3[0].page_content)\n",
        "print(docs3[0].metadata)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "GVGKCQRwdP3y"
      },
      "source": [
        "## Build RAG chain with Milvus Vector Store\n",
        "\n",
        "We will initialize a Milvus vector store with the documents, which load the documents into the Milvus vector store and build an index under the hood."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "is_executing": true,
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4AjxURZdP3y",
        "outputId": "cc9f9fb0-bbe7-48b0-8f7b-0875549405a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:pymilvus.milvus_client.milvus_client:Created new connection using: 650fc828dae242cba31d1cd454556bd1\n",
            "DEBUG:pymilvus.milvus_client.milvus_client:Created new connection using: 55acebea3d9b4d9d8ddf8eb526ce6c30\n",
            "DEBUG:pymilvus.milvus_client.milvus_client:Created new connection using: 60880d0b52da494eb01c84587b67f356\n"
          ]
        }
      ],
      "source": [
        "from langchain_milvus import Milvus, Zilliz\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "vectorstore = Milvus.from_documents(  # or Zilliz.from_documents\n",
        "    documents=docs,\n",
        "    embedding=embeddings,\n",
        "    collection_name=\"HI\",\n",
        "    connection_args={\n",
        "        \"uri\": \"./sanity.db\",\n",
        "    },\n",
        "      # Drop the old Milvus collection if it exists\n",
        ")\n",
        "\n",
        "policy_store = Milvus.from_documents(\n",
        "    documents=docs3,\n",
        "    embedding=embeddings,\n",
        "    collection_name=\"POLICY\",\n",
        "    connection_args={\n",
        "        \"uri\": \"./milvus_demo.db\",\n",
        "    },  # Drop the old Milvus collection if it exists\n",
        ")\n",
        "\n",
        "vectorbore=Milvus.from_documents(  # or Zilliz.from_documents\n",
        "    documents=docs2,\n",
        "    embedding=embeddings,\n",
        "    collection_name=\"BRUH\",\n",
        "    connection_args={\n",
        "        \"uri\": \"./milvus_demo.db\",\n",
        "    },  # Drop the old Milvus collection if it exists\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "xhy1LZYkdP3z"
      },
      "source": [
        "Search the documents in the Milvus vector store using a test query question. Let's take a look at the top 1 document."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "THE ROUGH WORKING OF AGENT2: Troubleshooting and getting to know more about the policy\n"
      ],
      "metadata": {
        "id": "d6FEfi6wjuWl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Unxzk4KsdP3z",
        "outputId": "d08bb003-f944-4e0c-eb18-06336f1cbf2d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'pk': 453792662664249405, 'seq_num': 2, 'source': '/content/adpolicy.json'}, page_content='{\"policy_type\": \"ad\", \"ad\": {\"ous\": {\"ou3\": [\"all\"], \"ou4\": [\"specific_ad_group3\"]}}, \"permissions\": {\"allowed\": false, \"iam_users\": [\"user3\"], \"iam_groups\": []}}')]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "query = \"Give me the policy only for user3\"\n",
        "policy_store.similarity_search(query, k=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "kBAL2jfKdP3z"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Initialize the OpenAI language model for response generation\n",
        "#llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "# Define the prompt template for generating AI responses\n",
        "PROMPT_TEMPLATE = \"\"\"\n",
        "Human: You are an AI assistant, and provides answers to questions by using fact based and statistical information when possible.\n",
        "Use the following pieces of information to provide a concise answer to the question enclosed in <question> tags.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "\n",
        "<question>\n",
        "{question}\n",
        "</question>\n",
        "\n",
        "The response should be specific and use statistics or numbers when possible.\n",
        "\n",
        "Assistant:\"\"\"\n",
        "\n",
        "# Create a PromptTemplate instance with the defined template and input variables\n",
        "prompt = PromptTemplate(\n",
        "    template=PROMPT_TEMPLATE, input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "# Convert the vector store to a retriever\n",
        "retriever = policy_store.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 1})\n",
        "\n",
        "\n",
        "# Define a function to format the retrieved documents\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "hvT74r5PdP3z"
      },
      "source": [
        "Use the LCEL(LangChain Expression Language) to build a RAG chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "cdLZs7YldP3z",
        "outputId": "f9bff4c5-d6c4-4c72-e6fe-d8aa832fab4a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The policy for user3 only allows access to specific_ad_group3 in the ad OU.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "# Define the RAG (Retrieval-Augmented Generation) chain for AI response generation\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# rag_chain.get_graph().print_ascii()\n",
        "\n",
        "# Invoke the RAG chain with a specific question and retrieve the response\n",
        "res = rag_chain.invoke(query)\n",
        "res"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TODO:\n",
        "- Create RAG Agent 1\n",
        "- Create RAG Agent 2"
      ],
      "metadata": {
        "id": "QKo7pxUJaVkL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AGENT 1) Work In progress"
      ],
      "metadata": {
        "id": "xgh9YNbYkETV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "json_schema = {\n",
        "  \"title\": \"Endpoint Access Policy\",\n",
        "  \"description\": \"Defines access control policy for endpoints, specifying allowed users and groups.\",\n",
        "  \"type\": \"object\",\n",
        "  \"properties\": {\n",
        "    \"policy_type\": {\n",
        "      \"type\": \"string\",\n",
        "      \"description\": \"Type of policy.  Currently only 'endpoints' is supported.\",\n",
        "      \"enum\": [\"endpoints\"]\n",
        "    },\n",
        "    \"endpoints\": {\n",
        "      \"type\": \"object\",\n",
        "      \"description\": \"Specifies endpoints and their grouping.\",\n",
        "      \"properties\": {\n",
        "        \"groups\": {\n",
        "          \"type\": \"object\",\n",
        "          \"description\": \"Groups of endpoints.\",\n",
        "          \"additionalProperties\": {\n",
        "            \"type\": \"array\",\n",
        "            \"items\": {\n",
        "              \"type\": \"string\",\n",
        "              \"description\": \"Endpoint names within the group.\"\n",
        "            }\n",
        "          }\n",
        "        },\n",
        "        \"users\": {\n",
        "          \"type\": \"array\",\n",
        "          \"items\": {\n",
        "            \"type\": \"string\",\n",
        "            \"description\": \"Individual users with access to all endpoints (if allowed).\"\n",
        "          }\n",
        "        }\n",
        "      },\n",
        "      \"required\": [\"groups\", \"users\"]\n",
        "    },\n",
        "    \"permissions\": {\n",
        "      \"type\": \"object\",\n",
        "      \"description\": \"Overall permission settings and IAM roles.\",\n",
        "      \"properties\": {\n",
        "        \"allowed\": {\n",
        "          \"type\": \"boolean\",\n",
        "          \"description\": \"Overall access allowed (true) or denied (false).  Overrides group/user settings if false.\"\n",
        "        },\n",
        "        \"iam_users\": {\n",
        "          \"type\": \"array\",\n",
        "          \"items\": {\n",
        "            \"type\": \"string\",\n",
        "            \"description\": \"IAM users with administrative control over this policy.\"\n",
        "          }\n",
        "        },\n",
        "        \"iam_groups\": {\n",
        "          \"type\": \"array\",\n",
        "          \"items\": {\n",
        "            \"type\": \"string\",\n",
        "            \"description\": \"IAM groups with administrative control over this policy.\"\n",
        "          }\n",
        "        }\n",
        "      },\n",
        "      \"required\": [\"allowed\", \"iam_users\", \"iam_groups\"]\n",
        "    }\n",
        "  },\n",
        "  \"required\": [\"policy_type\", \"endpoints\", \"permissions\"]\n",
        "}\n"
      ],
      "metadata": {
        "id": "VZpxtcDJWz3S"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Initialize the OpenAI language model for response generation\n",
        "#llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "# Define the prompt template for generating AI responses\n",
        "PROMPT_GEN = \"\"\" Human: You are an AI assistant that generates JSON access control policies. Here's an example of a valid policy:\n",
        "\n",
        "{{\n",
        "  \"policy_type\": \"endpoints\",\n",
        "  \"endpoints\": {{\n",
        "    \"groups\": {{\n",
        "      \"groupA\": [\"endpoint1\", \"endpoint2\"]\n",
        "    }},\n",
        "    \"users\": [\"user1\", \"user2\"]\n",
        "  }},\n",
        "  \"permissions\": {{\n",
        "    \"allowed\": true,\n",
        "    \"iam_users\": [\"admin1\", \"admin2\"],\n",
        "    \"iam_groups\": [\"admins\"]\n",
        "  }}\n",
        "}}\n",
        "Use the following authentication log data to generate a similar policy. Consider security best practices: users and groups with many failed logins should have restricted access.\n",
        "\n",
        "<context></context>\n",
        "<question>Generate a JSON access control policy based on the provided authentication logs.</question>\n",
        "\n",
        "Assistant:\"\"\"\n",
        "\n",
        "# Create a PromptTemplate instance with the defined template and input variables\n",
        "prompt = PromptTemplate(\n",
        "    template=PROMPT_GEN, input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "# Convert the vector store to a retriever\n",
        "retriever = policy_store.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 1})\n",
        "\n",
        "\n",
        "# Define a function to format the retrieved documents\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
      ],
      "metadata": {
        "id": "-jFYHGlylFXP"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | JsonOutputParser(json_schema=json_schema)\n",
        ")\n",
        "\n",
        "# rag_chain.get_graph().print_ascii()\n",
        "query = \"Generate a JSON access for\"\n",
        "# Invoke the RAG chain with a specific question and retrieve the response\n",
        "res = rag_chain.invoke(query)\n",
        "res"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5sZs04ui_y4",
        "outputId": "7e190c53-f182-4468-cb86-1be4c35e1158"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'policy_type': 'endpoints',\n",
              " 'endpoints': {'groups': {'groupA': ['endpoint1', 'endpoint2']},\n",
              "  'users': ['user1', 'user2']},\n",
              " 'permissions': {'allowed': False,\n",
              "  'iam_users': ['user3', 'user4'],\n",
              "  'iam_groups': ['groupB']}}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "toUsW8dfdP30"
      },
      "source": [
        "## Metadata filtering\n",
        "\n",
        "We can use the [Milvus Scalar Filtering Rules](https://milvus.io/docs/boolean.md) to filter the documents based on metadata. We have loaded the documents from two different sources, and we can filter the documents by the metadata `source`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrXU6Gy1dP30",
        "outputId": "df481b39-b7f8-49da-e685-f362b7b09627"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'pk': 449281835035555858})]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vectorstore.similarity_search(\n",
        "    \"What is CoT?\",\n",
        "    k=1,\n",
        "    expr=\"source == 'https://lilianweng.github.io/posts/2023-06-23-agent/'\",\n",
        ")\n",
        "\n",
        "# The same as:\n",
        "# vectorstore.as_retriever(search_kwargs=dict(\n",
        "#     k=1,\n",
        "#     expr=\"source == 'https://lilianweng.github.io/posts/2023-06-23-agent/'\",\n",
        "# )).invoke(\"What is CoT?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "k_Q9QKKydP30"
      },
      "source": [
        "If we want to dynamically change the search parameters without rebuilding the chain, we can [configure the runtime chain internals](https://python.langchain.com/v0.2/docs/how_to/configure/) . Let's define a new retriever with this dynamically configure and use it to build a new RAG chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRKzcv7QdP30",
        "outputId": "9c64e408-4c62-4ab9-a824-e0611243e502"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(page_content='Self-Reflection#\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\\nReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to generate reasoning traces in natural language.\\nThe ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:\\nThought: ...\\nAction: ...\\nObservation: ...\\n... (Repeated many times)', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'pk': 449281835035555859})]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.runnables import ConfigurableField\n",
        "\n",
        "# Define a new retriever with a configurable field for search_kwargs\n",
        "retriever2 = vectorstore.as_retriever().configurable_fields(\n",
        "    search_kwargs=ConfigurableField(\n",
        "        id=\"retriever_search_kwargs\",\n",
        "    )\n",
        ")\n",
        "\n",
        "# Invoke the retriever with a specific search_kwargs which filter the documents by source\n",
        "retriever2.with_config(\n",
        "    configurable={\n",
        "        \"retriever_search_kwargs\": dict(\n",
        "            expr=\"source == 'https://lilianweng.github.io/posts/2023-06-23-agent/'\",\n",
        "            k=1,\n",
        "        )\n",
        "    }\n",
        ").invoke(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "h0S8rhvadP30"
      },
      "outputs": [],
      "source": [
        "# Define a new RAG chain with this dynamically configurable retriever\n",
        "rag_chain2 = (\n",
        "    {\"context\": retriever2 | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "pSUGshXydP30"
      },
      "source": [
        "Let's try this dynamically configurable RAG chain with different filter conditions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_YIUjRzdP30",
        "outputId": "0bef6cac-0d74-45be-aa29-cb39a31c5500"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Self-reflection of an AI agent involves the process of synthesizing memories into higher-level inferences over time to guide the agent's future behavior. It serves as a mechanism to create higher-level summaries of past events. One approach to self-reflection involves prompting the language model with the 100 most recent observations and asking it to generate the 3 most salient high-level questions based on those observations. This process helps the AI agent optimize believability in the current moment and over time.\""
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Invoke this RAG chain with a specific question and config\n",
        "rag_chain2.with_config(\n",
        "    configurable={\n",
        "        \"retriever_search_kwargs\": dict(\n",
        "            expr=\"source == 'https://lilianweng.github.io/posts/2023-06-23-agent/'\",\n",
        "        )\n",
        "    }\n",
        ").invoke(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "AHG1PQecdP30"
      },
      "source": [
        "When we change the search condition to filter the documents by the second source, as the content of this blog source has nothing todo with the query question, we get an answer with no relevant information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wU2a2woxdP31",
        "outputId": "b3e04cb3-6e32-4bc4-da01-d635699de98a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"I'm sorry, but based on the provided context, there is no specific information or statistical data available regarding the self-reflection of an AI agent.\""
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rag_chain2.with_config(\n",
        "    configurable={\n",
        "        \"retriever_search_kwargs\": dict(\n",
        "            expr=\"source == 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/'\",\n",
        "        )\n",
        "    }\n",
        ").invoke(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "ac5H1BhodP31"
      },
      "source": [
        "----\n",
        "This tutorial focus the basic usage of Milvus LangChain integration and simple RAG approach. For more advanced RAG techniques, please refer to the [advanced rag bootcamp](https://github.com/milvus-io/bootcamp/tree/master/bootcamp/RAG/advanced_rag)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "toUsW8dfdP30"
      ],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}